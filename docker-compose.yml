services:
  # Base LLM service (CPU-only)
  llm_service:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
      - ./init.sh:/init.sh
    deploy:
      resources:
        limits:
          memory: 4G
    entrypoint: ["/bin/bash", "/init.sh"]
    profiles:
      - cpu

  # GPU-enabled LLM service
  llm_service_gpu:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_models:/root/.ollama
      - ./init.sh:/init.sh
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
        limits:
          memory: 4G
    entrypoint: ["/bin/bash", "/init.sh"]
    profiles:
      - gpu

  vector_db:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
    profiles:
      - cpu
      - gpu

  # RAG API service (CPU profile)
  rag_api:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "80:8000"
    volumes:
      - app_docs:/docs
    depends_on:
      - llm_service
      - vector_db
    environment:
      - OLLAMA_HOST=http://llm_service:11434
      - QDRANT_HOST=http://vector_db:6333
    profiles:
      - cpu

  # RAG API service (GPU profile)
  rag_api_gpu:
    build:
      context: .
      dockerfile: Dockerfile
    ports:
      - "80:8000"
    volumes:
      - app_docs:/docs
    depends_on:
      - llm_service_gpu
      - vector_db
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - OLLAMA_HOST=http://llm_service_gpu:11434
      - QDRANT_HOST=http://vector_db:6333
    profiles:
      - gpu

volumes:
  ollama_models:
  qdrant_data:
  app_docs:
